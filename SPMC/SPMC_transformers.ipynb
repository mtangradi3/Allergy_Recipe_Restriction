{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchinfo\n",
    "\n",
    "from torch import nn\n",
    "from torchvision import transforms"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:33.167417Z",
     "start_time": "2023-10-30T20:08:31.821332300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-30T20:08:43.402468Z",
     "start_time": "2023-10-30T20:08:43.398467700Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'cuda'"
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input directory 'dataset/train' does not exist.\n",
      "Conversion and replacement to RGBA complete.\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image, UnidentifiedImageError\n",
    "import os\n",
    "\n",
    "# Input directory\n",
    "input_dir = \"dataset/train\"\n",
    "\n",
    "# Ensure the input directory exists\n",
    "if not os.path.exists(input_dir):\n",
    "    print(f\"Input directory '{input_dir}' does not exist.\")\n",
    "    exit(1)\n",
    "\n",
    "# Recursive conversion function\n",
    "def convert_images_in_directory(directory):\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file_name in files:\n",
    "            if file_name.lower().endswith((\".png\", \".jpg\", \".jpeg\")):\n",
    "                file_path = os.path.join(root, file_name)\n",
    "\n",
    "                try:\n",
    "                    # Open the image\n",
    "                    image = Image.open(file_path)\n",
    "\n",
    "                    # Check if the image is already in RGBA format\n",
    "                    if image.mode == \"RGBA\":\n",
    "                        print(f\"Skipping {file_path} (already in RGBA format).\")\n",
    "                        continue\n",
    "\n",
    "                    # Convert the image to RGBA format\n",
    "                    image = image.convert(\"RGBA\")\n",
    "\n",
    "                    # Generate the output RGBA file name\n",
    "                    rgba_file = os.path.splitext(file_name)[0] + \"_rgba.png\"\n",
    "\n",
    "                    # Save the image as RGBA format, replacing the original file\n",
    "                    output_path = os.path.join(root, rgba_file)\n",
    "                    image.save(output_path)\n",
    "\n",
    "                    # Remove the original file\n",
    "                    os.remove(file_path)\n",
    "\n",
    "                    print(f\"Converted {file_path} to {output_path}\")\n",
    "\n",
    "                except (OSError, UnidentifiedImageError) as e:\n",
    "                    print(f\"Skipping {file_path} due to an error: {e}\")\n",
    "\n",
    "# Start the conversion process from the input directory\n",
    "convert_images_in_directory(input_dir)\n",
    "\n",
    "print(\"Conversion and replacement to RGBA complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-30T21:17:32.500395700Z",
     "start_time": "2023-10-30T21:17:32.489881700Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"dataset/train\"\n",
    "test_dir = \"dataset/test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "NUM_WORKERS= os.cpu_count()\n",
    "\n",
    "def create_dataloaders(\n",
    "        train_dir: str,\n",
    "        test_dir: str,\n",
    "        transform: transforms.Compose,\n",
    "        batch_size: int,\n",
    "        num_workers: int = NUM_WORKERS\n",
    "):\n",
    "    train_data = datasets.ImageFolder(train_dir, transform=transform)\n",
    "    test_data = datasets.ImageFolder(test_dir, transform=transform)\n",
    "    \n",
    "    class_names = train_data.classes\n",
    "    \n",
    "    train_dataloader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    test_dataloader = DataLoader(\n",
    "        test_data,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,\n",
    "        pin_memory=True\n",
    "    )\n",
    "    \n",
    "    return train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "IMG_SIZE = 224\n",
    "manual_transforms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)), transforms.ToTensor(),\n",
    "])\n",
    "print(f\"manually created transforms: {manual_transforms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "train_dataloader, test_dataloader, class_names = create_dataloaders(\n",
    "    train_dir=train_dir,\n",
    "    test_dir=test_dir,\n",
    "    transform=manual_transforms, \n",
    "    batch_size=BATCH_SIZE)\n",
    "train_dataloader, test_dataloader, class_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "image_batch, label_batch = next(iter(train_dataloader))\n",
    "image, label = image_batch[0], label_batch[0]\n",
    "print(image.shape, label)\n",
    "plt.imshow(image.permute(1,2,0))\n",
    "plt.title(class_names[label])\n",
    "plt.axis(False);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class PatchEmbedding(nn.Module):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 embedding_dim:int=768):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
    "                                 out_channels=embedding_dim,\n",
    "                                 kernel_size=patch_size,\n",
    "                                 stride=patch_size,\n",
    "                                 padding=0)\n",
    "\n",
    "        self.flatten = nn.Flatten(start_dim=2,\n",
    "                                  end_dim=3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        image_resolution = x.shape[-1]\n",
    "        assert image_resolution % patch_size == 0, f\"Input image size must be divisble by patch size, image shape: {image_resolution}, patch size: {patch_size}\"\n",
    "        \n",
    "        x_patched = self.patcher(x)\n",
    "        x_flattened = self.flatten(x_patched) \n",
    "        \n",
    "        return x_flattened.permute(0, 2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "patch_size = 16\n",
    "def set_seeds(seed: int=42):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    \n",
    "set_seeds()\n",
    "patchify = PatchEmbedding(in_channels=3,\n",
    "                              patch_size=16,\n",
    "                              embedding_dim=768)\n",
    "print(f\"Input image shape: {image.unsqueeze(0).shape}\")\n",
    "patch_embedded_image = patchify(image.unsqueeze(0))\n",
    "print(f\"Output patch embedding shape: {patch_embedded_image.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "set_seeds()\n",
    "patch_size = 16\n",
    "print(f\"Image tensor shape: {image.shape}\")\n",
    "height, width = image.shape[1], image.shape[2]\n",
    "x = image.unsqueeze(0)\n",
    "print(f\"Input image with batch dimension shape: {x.shape}\")\n",
    "patch_embedding_layer = PatchEmbedding(in_channels=3,\n",
    "                                       patch_size=patch_size,\n",
    "                                       embedding_dim=768)\n",
    "patch_embedding = patch_embedding_layer(x)\n",
    "print(f\"Patching embedding shape: {patch_embedding.shape}\")\n",
    "batch_size = patch_embedding.shape[0]\n",
    "\n",
    "embedding_dimension = patch_embedding.shape[-1]\n",
    "\n",
    "class_token = nn.Parameter(torch.ones(batch_size, 1, embedding_dimension), requires_grad=True) \n",
    "\n",
    "print(f\"Class token embedding shape: {class_token.shape}\")\n",
    "\n",
    "patch_embedding_class_token = torch.cat((class_token, patch_embedding), dim=1)\n",
    "\n",
    "print(f\"Patch embedding with class token shape: {patch_embedding_class_token.shape}\")\n",
    "\n",
    "number_of_patches = int((height * width) / patch_size**2)\n",
    "\n",
    "position_embedding = nn.Parameter(torch.ones(1, number_of_patches+1, embedding_dimension),\n",
    "                                  requires_grad=True)\n",
    "patch_and_position_embedding = patch_embedding_class_token + position_embedding\n",
    "print(f\"Patch and position embedding shape: {patch_and_position_embedding.shape}\")\n",
    "\n",
    "print(patch_embedding_class_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MultiheadSelfAttentionBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, \n",
    "                 num_heads:int=12, \n",
    "                 attn_dropout:float=0): \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "        \n",
    "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
    "                                                    num_heads=num_heads,\n",
    "                                                    dropout=attn_dropout,\n",
    "                                                    batch_first=True) \n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        attn_output, _ = self.multihead_attn(query=x, \n",
    "                                             key=x,\n",
    "                                             value=x,\n",
    "                                             need_weights=False) \n",
    "        return attn_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class MLPBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, \n",
    "                 mlp_size:int=3072, \n",
    "                 dropout:float=0.1): \n",
    "        super().__init__()\n",
    "        \n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
    "        \n",
    "       \n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_features=embedding_dim,\n",
    "                      out_features=mlp_size),\n",
    "            nn.GELU(), \n",
    "            nn.Dropout(p=dropout),\n",
    "            nn.Linear(in_features=mlp_size, \n",
    "                      out_features=embedding_dim), \n",
    "            nn.Dropout(p=dropout) \n",
    "        )\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer_norm(x)\n",
    "        x = self.mlp(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class TransformerEncoderBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embedding_dim:int=768, \n",
    "                 num_heads:int=12, \n",
    "                 mlp_size:int=3072, \n",
    "                 mlp_dropout:float=0.1, \n",
    "                 attn_dropout:float=0): \n",
    "        super().__init__()\n",
    "\n",
    "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
    "                                                     num_heads=num_heads,\n",
    "                                                     attn_dropout=attn_dropout)\n",
    "        \n",
    "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
    "                                   mlp_size=mlp_size,\n",
    "                                   dropout=mlp_dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x =  self.msa_block(x) + x \n",
    "        \n",
    "        x = self.mlp_block(x) + x \n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "transformer_encoder_block = TransformerEncoderBlock()\n",
    "\n",
    "from torchinfo import summary\n",
    "summary(model=transformer_encoder_block,\n",
    "        input_size=(1, 197, 768), \n",
    "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
    "        col_width=20,\n",
    "       row_settings=[\"var_names\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ViT(nn.Module):\n",
    "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
    "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
    "    def __init__(self,\n",
    "                 img_size:int=224, \n",
    "                 in_channels:int=3,\n",
    "                 patch_size:int=16,\n",
    "                 num_transformer_layers:int=12, \n",
    "                 embedding_dim:int=768, \n",
    "                 mlp_size:int=3072, \n",
    "                 num_heads:int=12, \n",
    "                 attn_dropout:float=0, \n",
    "                 mlp_dropout:float=0.1, \n",
    "                 embedding_dropout:float=0.1, \n",
    "                 num_classes:int=1000): \n",
    "        super().__init__() \n",
    "        \n",
    "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
    "        \n",
    "        self.num_patches = (img_size * img_size) // patch_size**2\n",
    "                 \n",
    "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
    "                                            requires_grad=True)\n",
    "        \n",
    "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
    "                                               requires_grad=True)\n",
    "                \n",
    "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
    "        \n",
    "\n",
    "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
    "                                              patch_size=patch_size,\n",
    "                                              embedding_dim=embedding_dim)\n",
    "        \n",
    "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
    "                                                                            num_heads=num_heads,\n",
    "                                                                            mlp_size=mlp_size,\n",
    "                                                                            mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
    "       \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
    "            nn.Linear(in_features=embedding_dim, \n",
    "                      out_features=num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        batch_size = x.shape[0]\n",
    "        \n",
    "        class_token = self.class_embedding.expand(batch_size, -1, -1) \n",
    "\n",
    "        x = self.patch_embedding(x)\n",
    "\n",
    "        x = torch.cat((class_token, x), dim=1)\n",
    "\n",
    "        x = self.position_embedding + x\n",
    "       \n",
    "        x = self.embedding_dropout(x)\n",
    "\n",
    "        x = self.transformer_encoder(x)\n",
    "\n",
    "        x = self.classifier(x[:, 0])\n",
    "\n",
    "        return x   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vit = ViT(num_classes=len(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from going_modular.going_modular import engine\n",
    "\n",
    "# Setup the optimizer to optimize our ViT model parameters using hyperparameters from the ViT paper \n",
    "optimizer = torch.optim.Adam(params=vit.parameters(), \n",
    "                             lr=3e-3,\n",
    "                             betas=(0.9, 0.999),\n",
    "                             weight_decay=0.3) \n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "set_seeds()\n",
    "\n",
    "results = engine.train(model=vit,\n",
    "                       train_dataloader=train_dataloader,\n",
    "                       test_dataloader=test_dataloader,\n",
    "                       optimizer=optimizer,\n",
    "                       loss_fn=loss_fn,\n",
    "                       epochs=25,\n",
    "                       device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from helper_functions import plot_loss_curves\n",
    "\n",
    "plot_loss_curves(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "\n",
    "from going_modular.going_modular.predictions import pred_and_plot_image\n",
    "\n",
    "\n",
    "custom_image_path = \"test_img.jpg\"\n",
    "\n",
    "pred_and_plot_image(model=vit,\n",
    "                    image_path=custom_image_path,\n",
    "                    class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
